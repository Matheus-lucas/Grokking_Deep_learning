{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9  Modeling Probabilities and Non-Linearities : Activation Functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOg4oIQvw4hswf4FUXXsqry",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matheus-lucas/Grokking_Deep_learning/blob/master/9_Modeling_Probabilities_and_Non_Linearities_Activation_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt62MXA8HW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho5Vm4hBTfwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLAE5dk7grfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tanh(x):  \n",
        "  return np.tanh(x)  \n",
        "\n",
        "def tanh2deriv(output):  \n",
        "  return 1 - (output ** 2)  \n",
        "\n",
        "def softmax(x):  \n",
        "  temp = np.exp(x)  \n",
        "  return (temp / np.sum(temp, axis=1, keepdims=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-zFK9P7LoRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "9c4b1e54-2a6f-416f-ab2f-6b2803a52581"
      },
      "source": [
        "np.random.seed(1)\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()  \n",
        "\n",
        "images, labels = (x_train[0:1000].reshape(1000,28*28) \\\n",
        "                  /255, y_train[0:1000])  \n",
        "\n",
        "one_hot_labels = np.zeros((len(labels),10))  \n",
        "\n",
        "for i,l in enumerate(labels):  \n",
        "  one_hot_labels[i][l] = 1  \n",
        "labels = one_hot_labels  \n",
        "\n",
        "test_images = x_test.reshape(len(x_test),28*28) / 255  \n",
        "test_labels = np.zeros((len(y_test),10))  \n",
        "for i,l in enumerate(y_test):  \n",
        "  test_labels[i][l] = 1  \n",
        "\n",
        "alpha, iterations, hidden_size = (2, 300, 100)  \n",
        "pixels_per_image, num_labels = (784, 10)  \n",
        "batch_size = 100  \n",
        "\n",
        "weights_0_1 = 0.02*np.random.random((pixels_per_image,hidden_size))- 0.01  \n",
        "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1  \n",
        "\n",
        "for j in range(iterations):  \n",
        "  correct_cnt = 0  \n",
        "  for i in range(int(len(images) / batch_size)):  \n",
        "    batch_start, batch_end=((i * batch_size),((i+1)*batch_size)) \n",
        "\n",
        "    layer_0 = images[batch_start:batch_end]\n",
        "\n",
        "    # keep the outputs in probabilities(0-1) using tanh\n",
        "    layer_1 = tanh(np.dot(layer_0,weights_0_1))\n",
        "\n",
        "    dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
        "\n",
        "    layer_1 *= dropout_mask * 2  \n",
        "\n",
        "    # using the softmax(exponencial functions e^x) to show the most probability correct answer in final layer\n",
        "    layer_2 = softmax(np.dot(layer_1,weights_1_2))\n",
        "    \n",
        "    # counting the correct answer beffore update the weights to measure how much the actual weights give correct answers\n",
        "    for k in range(batch_size):  \n",
        "        correct_cnt += int(np.argmax(layer_2[k:k+1])  == np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
        "        \n",
        "    layer_2_delta = (labels[batch_start:batch_end]- layer_2)/(batch_size * layer_2.shape[0]) \n",
        "\n",
        "    # use gradient descendent with tanh derivative\n",
        "    layer_1_delta = layer_2_delta.dot(weights_1_2.T) * tanh2deriv(layer_1)  \n",
        "    layer_1_delta *= dropout_mask  \n",
        "    \n",
        "    weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)  \n",
        "    weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)  \n",
        " \n",
        "  test_correct_cnt = 0    \n",
        "  for i in range(len(test_images)):  \n",
        "    layer_0 = test_images[i:i+1]  \n",
        "    layer_1 = tanh(np.dot(layer_0,weights_0_1))  \n",
        "    layer_2 = np.dot(layer_1,weights_1_2)  \n",
        "    test_correct_cnt += int(np.argmax(layer_2) == \\\n",
        "                          np.argmax(test_labels[i:i+1])) \n",
        "  if (j % 10 == 0): \n",
        "    # print the results \n",
        "    print(\"I:\" + str(j)+ \\\n",
        "      \" Test- Acc:\" + str(test_correct_cnt/ float(len(test_images)))+ \\\n",
        "      \" Train-ACC:\" + str(correct_cnt/ float(len(images)))[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I:0 Test- Acc:0.394 Train-ACC:0.156\n",
            "I:10 Test- Acc:0.6867 Train-ACC:0.723\n",
            "I:20 Test- Acc:0.7025 Train-ACC:0.732\n",
            "I:30 Test- Acc:0.734 Train-ACC:0.763\n",
            "I:40 Test- Acc:0.7663 Train-ACC:0.794\n",
            "I:50 Test- Acc:0.7913 Train-ACC:0.819\n",
            "I:60 Test- Acc:0.8102 Train-ACC:0.849\n",
            "I:70 Test- Acc:0.8228 Train-ACC:0.864\n",
            "I:80 Test- Acc:0.831 Train-ACC:0.867\n",
            "I:90 Test- Acc:0.8364 Train-ACC:0.885\n",
            "I:100 Test- Acc:0.8407 Train-ACC:0.883\n",
            "I:110 Test- Acc:0.845 Train-ACC:0.891\n",
            "I:120 Test- Acc:0.8481 Train-ACC:0.901\n",
            "I:130 Test- Acc:0.8505 Train-ACC:0.901\n",
            "I:150 Test- Acc:0.8555 Train-ACC:0.914\n",
            "I:160 Test- Acc:0.8577 Train-ACC:0.925\n",
            "I:170 Test- Acc:0.8596 Train-ACC:0.918\n",
            "I:180 Test- Acc:0.8619 Train-ACC:0.933\n",
            "I:190 Test- Acc:0.863 Train-ACC:0.933\n",
            "I:200 Test- Acc:0.8642 Train-ACC:0.926\n",
            "I:210 Test- Acc:0.8653 Train-ACC:0.931\n",
            "I:220 Test- Acc:0.8668 Train-ACC:0.93\n",
            "I:230 Test- Acc:0.8672 Train-ACC:0.937\n",
            "I:240 Test- Acc:0.8681 Train-ACC:0.938\n",
            "I:250 Test- Acc:0.8687 Train-ACC:0.937\n",
            "I:260 Test- Acc:0.8684 Train-ACC:0.945\n",
            "I:270 Test- Acc:0.8703 Train-ACC:0.951\n",
            "I:280 Test- Acc:0.8699 Train-ACC:0.949\n",
            "I:290 Test- Acc:0.8701 Train-ACC:0.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozBHyLlkebMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}